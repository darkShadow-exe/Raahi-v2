{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a66c305",
   "metadata": {},
   "source": [
    "# Raahi-v2 Transit RL Training on Google Colab\n",
    "\n",
    "This notebook trains reinforcement learning agents for transit optimization using synthetic city data.\n",
    "Run on Google Colab with GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721912de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install tensorboard\n",
    "!pip install gymnasium\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e26968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b180799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# City Builder class for generating training data\n",
    "@dataclass\n",
    "class Station:\n",
    "    id: str\n",
    "    x: int\n",
    "    y: int\n",
    "    station_type: str\n",
    "    is_transfer: bool = False\n",
    "\n",
    "class CityBuilder:\n",
    "    def __init__(self, grid_size: int = 8):\n",
    "        self.grid_size = grid_size\n",
    "        self.zone_types = [\"residential\", \"commercial\", \"industrial\"]\n",
    "        self.station_types = [\"bus\", \"tram\", \"metro\"]\n",
    "    \n",
    "    def generate_random_city(self) -> Dict:\n",
    "        zones = self._generate_zones()\n",
    "        stations = self._generate_stations()\n",
    "        connections = self._generate_connections(stations)\n",
    "        routes = self._generate_routes(stations)\n",
    "        \n",
    "        return {\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"zones\": zones,\n",
    "            \"stations\": stations,\n",
    "            \"connections\": connections,\n",
    "            \"routes\": routes\n",
    "        }\n",
    "    \n",
    "    def _generate_zones(self) -> List[Dict]:\n",
    "        zones = []\n",
    "        for x in range(self.grid_size):\n",
    "            for y in range(self.grid_size):\n",
    "                zone_type = random.choice(self.zone_types)\n",
    "                zones.append({\"x\": x, \"y\": y, \"type\": zone_type})\n",
    "        return zones\n",
    "    \n",
    "    def _generate_stations(self) -> List[Dict]:\n",
    "        stations = []\n",
    "        num_stations = random.randint(5, 15)\n",
    "        \n",
    "        for i in range(num_stations):\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            station_type = random.choice(self.station_types)\n",
    "            \n",
    "            stations.append({\n",
    "                \"id\": f\"station_{i}\",\n",
    "                \"x\": x, \"y\": y,\n",
    "                \"type\": station_type,\n",
    "                \"is_transfer\": False\n",
    "            })\n",
    "        return stations\n",
    "    \n",
    "    def _generate_connections(self, stations: List[Dict]) -> List[Dict]:\n",
    "        connections = []\n",
    "        for i, s1 in enumerate(stations):\n",
    "            for j, s2 in enumerate(stations[i+1:], i+1):\n",
    "                dist = abs(s1[\"x\"] - s2[\"x\"]) + abs(s1[\"y\"] - s2[\"y\"])\n",
    "                if dist <= 3:\n",
    "                    connections.append({\n",
    "                        \"from\": s1[\"id\"],\n",
    "                        \"to\": s2[\"id\"],\n",
    "                        \"walk_time\": dist * 60\n",
    "                    })\n",
    "        return connections\n",
    "    \n",
    "    def _generate_routes(self, stations: List[Dict]) -> List[Dict]:\n",
    "        routes = []\n",
    "        station_groups = defaultdict(list)\n",
    "        \n",
    "        for station in stations:\n",
    "            station_groups[station[\"type\"]].append(station)\n",
    "        \n",
    "        route_id = 0\n",
    "        for mode, mode_stations in station_groups.items():\n",
    "            if len(mode_stations) >= 2:\n",
    "                routes.append({\n",
    "                    \"id\": f\"route_{route_id}\",\n",
    "                    \"mode\": mode,\n",
    "                    \"stations\": [s[\"id\"] for s in mode_stations[:4]],\n",
    "                    \"color\": f\"#{''.join([random.choice('0123456789ABCDEF') for _ in range(6)])}\"\n",
    "                })\n",
    "                route_id += 1\n",
    "        \n",
    "        return routes\n",
    "\n",
    "print(\"City Builder ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf38793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transit Environment for RL training with passenger demand\n",
    "class TransitEnv(gym.Env):\n",
    "    def __init__(self, grid_size=8, training_cities=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.city_builder = CityBuilder(grid_size)\n",
    "        self.training_cities = training_cities or []\n",
    "        self.city_index = 0\n",
    "        \n",
    "        # Action space: choose route frequency for each route (0-10)\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=0, high=10, shape=(10,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space: city state + time + demand\n",
    "        obs_size = grid_size * grid_size * 4 + 24 + 10  # zones + time + demand\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1, shape=(obs_size,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Use pre-generated cities if available, otherwise generate new\n",
    "        if self.training_cities:\n",
    "            self.city_data = self.training_cities[self.city_index % len(self.training_cities)]\n",
    "            self.city_index += 1\n",
    "        else:\n",
    "            self.city_data = self.city_builder.generate_random_city()\n",
    "            \n",
    "        self.current_step = 0\n",
    "        self.max_steps = 100\n",
    "        self.current_hour = random.randint(0, 23)  # Random start time\n",
    "        \n",
    "        # Generate passenger demand patterns\n",
    "        self._generate_demand_patterns()\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        return obs, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply actions (route frequencies)\n",
    "        route_frequencies = action\n",
    "        \n",
    "        # Update time and passenger demand\n",
    "        self.current_hour = (self.current_hour + 1) % 24\n",
    "        current_demand = self._get_current_demand()\n",
    "        \n",
    "        # Calculate reward based on efficiency and passenger satisfaction\n",
    "        reward = self._calculate_reward(route_frequencies, current_demand)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        truncated = False\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        return obs, reward, done, truncated, {}\n",
    "    \n",
    "    def _generate_demand_patterns(self):\n",
    "        \"\"\"Generate realistic passenger demand patterns\"\"\"\n",
    "        stations = self.city_data[\"stations\"]\n",
    "        zones = self.city_data[\"zones\"]\n",
    "        \n",
    "        # Create zone-based demand patterns\n",
    "        self.zone_demand = {}\n",
    "        for zone in zones:\n",
    "            zone_type = zone[\"type\"]\n",
    "            if zone_type == \"residential\":\n",
    "                # High morning outbound, evening inbound\n",
    "                self.zone_demand[(zone[\"x\"], zone[\"y\"])] = {\n",
    "                    \"morning_out\": random.uniform(0.7, 1.0),\n",
    "                    \"evening_in\": random.uniform(0.7, 1.0)\n",
    "                }\n",
    "            elif zone_type == \"commercial\":\n",
    "                # High morning inbound, evening outbound\n",
    "                self.zone_demand[(zone[\"x\"], zone[\"y\"])] = {\n",
    "                    \"morning_in\": random.uniform(0.6, 0.9),\n",
    "                    \"evening_out\": random.uniform(0.6, 0.9)\n",
    "                }\n",
    "            else:  # industrial\n",
    "                # Steady demand throughout day\n",
    "                self.zone_demand[(zone[\"x\"], zone[\"y\"])] = {\n",
    "                    \"steady\": random.uniform(0.3, 0.6)\n",
    "                }\n",
    "        \n",
    "        # Generate common OD pairs\n",
    "        self.od_pairs = []\n",
    "        for i, s1 in enumerate(stations[:8]):  # Limit to prevent too many pairs\n",
    "            for j, s2 in enumerate(stations[:8]):\n",
    "                if i != j:\n",
    "                    # Distance-based demand (closer = more likely)\n",
    "                    dist = abs(s1[\"x\"] - s2[\"x\"]) + abs(s1[\"y\"] - s2[\"y\"])\n",
    "                    demand_weight = max(0.1, 1.0 - dist / 10.0)\n",
    "                    \n",
    "                    self.od_pairs.append({\n",
    "                        \"origin\": s1[\"id\"],\n",
    "                        \"destination\": s2[\"id\"],\n",
    "                        \"base_demand\": random.uniform(0.1, demand_weight),\n",
    "                        \"peak_multiplier\": random.uniform(1.5, 3.0)\n",
    "                    })\n",
    "    \n",
    "    def _get_current_demand(self):\n",
    "        \"\"\"Get passenger demand for current time\"\"\"\n",
    "        hour = self.current_hour\n",
    "        \n",
    "        # Time-based demand multipliers\n",
    "        if 7 <= hour <= 9:  # Morning rush\n",
    "            time_multiplier = 2.0\n",
    "        elif 17 <= hour <= 19:  # Evening rush\n",
    "            time_multiplier = 2.0\n",
    "        elif 6 <= hour <= 22:  # Day time\n",
    "            time_multiplier = 1.0\n",
    "        else:  # Night\n",
    "            time_multiplier = 0.3\n",
    "        \n",
    "        # Calculate total demand\n",
    "        total_demand = 0\n",
    "        for od_pair in self.od_pairs:\n",
    "            base = od_pair[\"base_demand\"]\n",
    "            \n",
    "            # Add rush hour effects\n",
    "            if 7 <= hour <= 9 or 17 <= hour <= 19:\n",
    "                demand = base * od_pair[\"peak_multiplier\"] * time_multiplier\n",
    "            else:\n",
    "                demand = base * time_multiplier\n",
    "            \n",
    "            # Add random variation\n",
    "            demand *= random.uniform(0.8, 1.2)\n",
    "            total_demand += demand\n",
    "        \n",
    "        return min(total_demand, 10.0)  # Cap at 10\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        # Convert city data to observation vector\n",
    "        obs = np.zeros(self.grid_size * self.grid_size * 4 + 24 + 10, dtype=np.float32)\n",
    "        \n",
    "        # Encode zones\n",
    "        for zone in self.city_data[\"zones\"]:\n",
    "            idx = zone[\"x\"] * self.grid_size + zone[\"y\"]\n",
    "            if zone[\"type\"] == \"residential\":\n",
    "                obs[idx] = 1.0\n",
    "            elif zone[\"type\"] == \"commercial\":\n",
    "                obs[idx + self.grid_size * self.grid_size] = 1.0\n",
    "            elif zone[\"type\"] == \"industrial\":\n",
    "                obs[idx + 2 * self.grid_size * self.grid_size] = 1.0\n",
    "        \n",
    "        # Encode stations\n",
    "        for station in self.city_data[\"stations\"]:\n",
    "            idx = station[\"x\"] * self.grid_size + station[\"y\"]\n",
    "            obs[idx + 3 * self.grid_size * self.grid_size] = 1.0\n",
    "        \n",
    "        # Encode time of day (one-hot)\n",
    "        time_offset = self.grid_size * self.grid_size * 4\n",
    "        obs[time_offset + self.current_hour] = 1.0\n",
    "        \n",
    "        # Encode current demand levels\n",
    "        demand_offset = time_offset + 24\n",
    "        current_demand = self._get_current_demand()\n",
    "        for i in range(10):\n",
    "            if i < current_demand:\n",
    "                obs[demand_offset + i] = 1.0\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _calculate_reward(self, route_frequencies, current_demand):\n",
    "        # Advanced reward with passenger satisfaction\n",
    "        \n",
    "        # Service efficiency (frequency vs demand matching)\n",
    "        total_frequency = np.sum(route_frequencies)\n",
    "        demand_match = 1.0 - abs(total_frequency - current_demand) / 10.0\n",
    "        demand_reward = demand_match * 2.0\n",
    "        \n",
    "        # Coverage reward\n",
    "        coverage = len(self.city_data[\"stations\"]) * 0.02\n",
    "        \n",
    "        # Operating cost (quadratic penalty for high frequencies)\n",
    "        cost = np.sum(route_frequencies ** 2) * 0.03\n",
    "        \n",
    "        # Rush hour bonus for adequate service\n",
    "        hour = self.current_hour\n",
    "        if (7 <= hour <= 9 or 17 <= hour <= 19) and total_frequency >= current_demand:\n",
    "            rush_bonus = 1.0\n",
    "        else:\n",
    "            rush_bonus = 0.0\n",
    "        \n",
    "        # Passenger waiting time penalty (inverse of frequency)\n",
    "        wait_penalty = np.sum(1.0 / (route_frequencies + 0.1)) * 0.1\n",
    "        \n",
    "        reward = demand_reward + coverage + rush_bonus - cost - wait_penalty\n",
    "        return float(reward)\n",
    "\n",
    "print(\"Enhanced Transit Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training cities\n",
    "print(\"Generating training cities...\")\n",
    "builder = CityBuilder(15)  # 15x15 grid\n",
    "training_cities = []\n",
    "\n",
    "for i in range(50):\n",
    "    city = builder.generate_random_city()\n",
    "    training_cities.append(city)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Generated {i+1} cities\")\n",
    "\n",
    "print(f\"Generated {len(training_cities)} training cities\")\n",
    "print(f\"Sample city has {len(training_cities[0]['stations'])} stations\")\n",
    "print(f\"Grid size: {training_cities[0]['grid_size']}x{training_cities[0]['grid_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze demand patterns\n",
    "print(\"Demand Pattern Analysis:\")\n",
    "\n",
    "# Test environment with demand\n",
    "test_env = TransitEnv(grid_size=15, training_cities=training_cities[:1])\n",
    "obs, info = test_env.reset()\n",
    "\n",
    "print(f\"OD pairs generated: {len(test_env.od_pairs)}\")\n",
    "print(f\"Zone demand patterns: {len(test_env.zone_demand)}\")\n",
    "\n",
    "# Show demand variation over 24 hours\n",
    "hourly_demand = []\n",
    "for hour in range(24):\n",
    "    test_env.current_hour = hour\n",
    "    demand = test_env._get_current_demand()\n",
    "    hourly_demand.append(demand)\n",
    "\n",
    "print(f\"Peak demand: {max(hourly_demand):.2f}\")\n",
    "print(f\"Low demand: {min(hourly_demand):.2f}\")\n",
    "print(f\"Rush hours (7-9, 17-19): {[hourly_demand[i] for i in [7,8,9,17,18,19]]}\")\n",
    "\n",
    "# Plot demand curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(24), hourly_demand, marker='o')\n",
    "plt.title(\"Passenger Demand by Hour\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Demand Level\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d60595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with training cities\n",
    "env = TransitEnv(grid_size=15, training_cities=training_cities)\n",
    "print(f\"Environment created with {len(training_cities)} training cities\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# Test environment\n",
    "obs, info = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Current city has {len(env.city_data['stations'])} stations\")\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(f\"Test step reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745016ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./logs/\"\n",
    ")\n",
    "\n",
    "print(f\"PPO model created on {device}\")\n",
    "print(f\"Policy network: {model.policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff00dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "total_timesteps = 100000\n",
    "save_freq = 10000\n",
    "\n",
    "# Create callback for evaluation\n",
    "eval_env = TransitEnv(grid_size=8)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    best_model_save_path=\"./models/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Enhanced training with detailed logging\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "class MetricsCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_demands = []\n",
    "        self.episode_frequencies = []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Log episode data when episode ends\n",
    "        if self.locals['dones'][0]:\n",
    "            info = self.locals['infos'][0]\n",
    "            if 'episode' in info:\n",
    "                self.episode_rewards.append(info['episode']['r'])\n",
    "                self.episode_lengths.append(info['episode']['l'])\n",
    "        return True\n",
    "\n",
    "print(\"Starting enhanced training...\")\n",
    "total_timesteps = 100000\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"./models/\", exist_ok=True)\n",
    "os.makedirs(\"./logs/\", exist_ok=True)\n",
    "os.makedirs(\"./animations/\", exist_ok=True)\n",
    "\n",
    "# Enhanced callbacks\n",
    "metrics_callback = MetricsCallback()\n",
    "eval_env = TransitEnv(grid_size=15, training_cities=training_cities)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    best_model_save_path=\"./models/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train with enhanced logging\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=[eval_callback, metrics_callback],\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Save training metrics\n",
    "metrics_data = {\n",
    "    'episode_rewards': metrics_callback.episode_rewards,\n",
    "    'episode_lengths': metrics_callback.episode_lengths\n",
    "}\n",
    "\n",
    "with open('./logs/training_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "print(\"Enhanced training completed!\")\n",
    "print(f\"Total episodes: {len(metrics_callback.episode_rewards)}\")\n",
    "print(f\"Metrics saved to ./logs/training_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee968c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save(\"transit_ppo_final\")\n",
    "print(\"Model saved as 'transit_ppo_final'\")\n",
    "\n",
    "# Test trained model\n",
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Test episode reward: {total_reward}\")\n",
    "print(f\"Average reward per step: {total_reward / (step + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluate on multiple episodes\n",
    "num_eval_episodes = 20\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_eval_episodes):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(100):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    if episode % 5 == 0:\n",
    "        print(f\"Episode {episode}: {episode_reward:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rewards, bins=10)\n",
    "plt.title(\"Reward Distribution\")\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average reward: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")\n",
    "print(f\"Min reward: {np.min(rewards):.2f}\")\n",
    "print(f\"Max reward: {np.max(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a203dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model analysis\n",
    "print(\"Model Analysis:\")\n",
    "print(f\"Policy network: {model.policy}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.policy.parameters())}\")\n",
    "\n",
    "# Sample action analysis\n",
    "obs, info = env.reset()\n",
    "action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "print(f\"\\nSample action (route frequencies): {action}\")\n",
    "print(f\"Action statistics:\")\n",
    "print(f\"  Mean: {np.mean(action):.3f}\")\n",
    "print(f\"  Std: {np.std(action):.3f}\")\n",
    "print(f\"  Min: {np.min(action):.3f}\")\n",
    "print(f\"  Max: {np.max(action):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1667fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model files (for Colab)\n",
    "from google.colab import files\n",
    "\n",
    "try:\n",
    "    # Zip model files\n",
    "    !zip -r trained_models.zip transit_ppo_final.zip models/ logs/\n",
    "    \n",
    "    # Download\n",
    "    files.download('trained_models.zip')\n",
    "    print(\"Model files downloaded!\")\n",
    "except:\n",
    "    print(\"Not running on Colab, skipping download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cc044",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "The RL agent has been trained on synthetic transit data. Key results:\n",
    "\n",
    "- **Environment**: 15x15 grid cities with multiple transit modes\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Training**: 100K timesteps with GPU acceleration\n",
    "- **Objective**: Optimize route frequencies for efficiency\n",
    "\n",
    "The trained model can now make decisions about transit route scheduling to maximize efficiency while minimizing costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ce43b",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "The evaluation results show:\n",
    "- Average reward: -293.88 ± 65.10\n",
    "- Min reward: -391.61\n",
    "- Max reward: -160.87\n",
    "\n",
    "These negative rewards are typical in RL problems with cost penalties. The large standard deviation (65.10) indicates inconsistent performance across different cities/scenarios.\n",
    "\n",
    "### Potential Improvements\n",
    "\n",
    "1. **Extended Training:** 100K timesteps may not be enough for a complex RL task. Try 500K+ timesteps.\n",
    "\n",
    "2. **Reward Function Tuning:** The current reward calculation might penalize certain actions too heavily.\n",
    "\n",
    "3. **Hyperparameter Optimization:** Adjust learning rate, batch size, or network architecture.\n",
    "\n",
    "4. **Curriculum Learning:** Start with simpler cities and gradually increase complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended Training and Parameter Tuning\n",
    "print(\"Starting extended training...\")\n",
    "\n",
    "# Create improved model with tuned parameters\n",
    "improved_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-4,  # Lower learning rate for stability\n",
    "    n_steps=2048,\n",
    "    batch_size=128,      # Larger batch size\n",
    "    n_epochs=15,         # More epochs per update\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.005,      # Lower entropy for exploitation\n",
    "    policy_kwargs={\"net_arch\": [256, 256]},  # Deeper network\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./logs/\"\n",
    ")\n",
    "\n",
    "# Uncomment to run extended training (takes longer)\n",
    "# total_timesteps = 500000  # 5x more training\n",
    "\n",
    "# Extended training (optional - takes longer)\n",
    "'''\n",
    "improved_model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=[eval_callback, metrics_callback],\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Save improved model\n",
    "improved_model.save(\"transit_ppo_improved\")\n",
    "print(\"Improved model saved as 'transit_ppo_improved'\")\n",
    "\n",
    "# Quick test of improved model\n",
    "obs, info = env.reset()\n",
    "improved_reward = 0\n",
    "for step in range(100):\n",
    "    action, _states = improved_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    improved_reward += reward\n",
    "    if done or truncated:\n",
    "        break\n",
    "print(f\"Improved model reward: {improved_reward:.2f}\")\n",
    "'''\n",
    "\n",
    "# Reward function analysis\n",
    "print(\"\\nReward Function Component Analysis:\")\n",
    "obs, info = env.reset()\n",
    "action, _ = model.predict(obs, deterministic=True)\n",
    "current_demand = env._get_current_demand()\n",
    "\n",
    "# Calculate individual reward components\n",
    "total_frequency = np.sum(action)\n",
    "demand_match = 1.0 - abs(total_frequency - current_demand) / 10.0\n",
    "demand_reward = demand_match * 2.0\n",
    "coverage = len(env.city_data[\"stations\"]) * 0.02\n",
    "cost = np.sum(action ** 2) * 0.03\n",
    "hour = env.current_hour\n",
    "rush_bonus = 1.0 if (7 <= hour <= 9 or 17 <= hour <= 19) and total_frequency >= current_demand else 0.0\n",
    "wait_penalty = np.sum(1.0 / (action + 0.1)) * 0.1\n",
    "\n",
    "print(f\"Current demand: {current_demand:.2f}\")\n",
    "print(f\"Total frequency: {total_frequency:.2f}\")\n",
    "print(f\"Demand match reward: {demand_reward:.2f}\")\n",
    "print(f\"Coverage reward: {coverage:.2f}\")\n",
    "print(f\"Operating cost penalty: {-cost:.2f}\")\n",
    "print(f\"Rush hour bonus: {rush_bonus:.2f}\")\n",
    "print(f\"Wait time penalty: {-wait_penalty:.2f}\")\n",
    "print(f\"Total reward: {demand_reward + coverage + rush_bonus - cost - wait_penalty:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reward components across different demand scenarios\n",
    "print(\"Visualizing reward components...\")\n",
    "\n",
    "test_hours = [0, 8, 12, 18]  # Night, morning rush, day, evening rush\n",
    "test_actions = [\n",
    "    np.array([1]*10),         # Low frequency across all routes\n",
    "    np.array([5]*10),         # Medium frequency\n",
    "    np.array([10]*10),        # Maximum frequency\n",
    "    np.array([8,8,8,8,8,2,2,2,2,2])  # Mixed frequency\n",
    "]\n",
    "\n",
    "reward_components = []\n",
    "\n",
    "for hour in test_hours:\n",
    "    env.reset()\n",
    "    env.current_hour = hour\n",
    "    current_demand = env._get_current_demand()\n",
    "    \n",
    "    for action in test_actions:\n",
    "        # Calculate reward components\n",
    "        total_frequency = np.sum(action)\n",
    "        demand_match = 1.0 - abs(total_frequency - current_demand) / 10.0\n",
    "        demand_reward = demand_match * 2.0\n",
    "        coverage = len(env.city_data[\"stations\"]) * 0.02\n",
    "        cost = np.sum(action ** 2) * 0.03\n",
    "        rush_bonus = 1.0 if (7 <= hour <= 9 or 17 <= hour <= 19) and total_frequency >= current_demand else 0.0\n",
    "        wait_penalty = np.sum(1.0 / (action + 0.1)) * 0.1\n",
    "        total = demand_reward + coverage + rush_bonus - cost - wait_penalty\n",
    "        \n",
    "        reward_components.append({\n",
    "            'hour': hour,\n",
    "            'action_type': ['Low', 'Medium', 'Maximum', 'Mixed'][test_actions.index(action)],\n",
    "            'demand': current_demand,\n",
    "            'frequency': total_frequency,\n",
    "            'demand_match': demand_reward,\n",
    "            'coverage': coverage,\n",
    "            'cost': -cost,\n",
    "            'rush_bonus': rush_bonus,\n",
    "            'wait_penalty': -wait_penalty,\n",
    "            'total': total\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(reward_components)\n",
    "\n",
    "# Plot reward breakdown by hour and action type\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Time labels\n",
    "time_labels = {0: \"Night\", 8: \"Morning Rush\", 12: \"Day\", 18: \"Evening Rush\"}\n",
    "\n",
    "# Loop through hours and create subplots\n",
    "for i, hour in enumerate(test_hours):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    hour_data = df[df['hour'] == hour]\n",
    "    \n",
    "    # Create bar chart for each component\n",
    "    bar_width = 0.15\n",
    "    positions = np.arange(len(hour_data))\n",
    "    \n",
    "    plt.bar(positions - bar_width*2, hour_data['demand_match'], bar_width, label='Demand Match')\n",
    "    plt.bar(positions - bar_width, hour_data['coverage'], bar_width, label='Coverage')\n",
    "    plt.bar(positions, hour_data['rush_bonus'], bar_width, label='Rush Bonus')\n",
    "    plt.bar(positions + bar_width, hour_data['cost'], bar_width, label='Cost Penalty')\n",
    "    plt.bar(positions + bar_width*2, hour_data['wait_penalty'], bar_width, label='Wait Penalty')\n",
    "    \n",
    "    plt.plot(positions, hour_data['total'], 'ko-', label='Total Reward')\n",
    "    \n",
    "    plt.title(f\"Time: {time_labels[hour]} (Hour {hour}), Demand: {hour_data['demand'].iloc[0]:.2f}\")\n",
    "    plt.xticks(positions, hour_data['action_type'])\n",
    "    plt.xlabel(\"Service Frequency Strategy\")\n",
    "    plt.ylabel(\"Reward Component Value\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Reward visualization complete!\")\n",
    "print(\"You can now adjust the reward function components to improve model performance.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
