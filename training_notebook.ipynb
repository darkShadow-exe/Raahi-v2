{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a66c305",
   "metadata": {},
   "source": [
    "# Raahi-v2 Transit RL Training on Google Colab\n",
    "\n",
    "This notebook trains reinforcement learning agents for transit optimization using synthetic city data.\n",
    "Run on Google Colab with GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721912de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install tensorboard\n",
    "!pip install gymnasium\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e26968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b180799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# City Builder class for generating training data\n",
    "@dataclass\n",
    "class Station:\n",
    "    id: str\n",
    "    x: int\n",
    "    y: int\n",
    "    station_type: str\n",
    "    is_transfer: bool = False\n",
    "\n",
    "class CityBuilder:\n",
    "    def __init__(self, grid_size: int = 8):\n",
    "        self.grid_size = grid_size\n",
    "        self.zone_types = [\"residential\", \"commercial\", \"industrial\"]\n",
    "        self.station_types = [\"bus\", \"tram\", \"metro\"]\n",
    "    \n",
    "    def generate_random_city(self) -> Dict:\n",
    "        zones = self._generate_zones()\n",
    "        stations = self._generate_stations()\n",
    "        connections = self._generate_connections(stations)\n",
    "        routes = self._generate_routes(stations)\n",
    "        \n",
    "        return {\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"zones\": zones,\n",
    "            \"stations\": stations,\n",
    "            \"connections\": connections,\n",
    "            \"routes\": routes\n",
    "        }\n",
    "    \n",
    "    def _generate_zones(self) -> List[Dict]:\n",
    "        zones = []\n",
    "        for x in range(self.grid_size):\n",
    "            for y in range(self.grid_size):\n",
    "                zone_type = random.choice(self.zone_types)\n",
    "                zones.append({\"x\": x, \"y\": y, \"type\": zone_type})\n",
    "        return zones\n",
    "    \n",
    "    def _generate_stations(self) -> List[Dict]:\n",
    "        stations = []\n",
    "        num_stations = random.randint(5, 15)\n",
    "        \n",
    "        for i in range(num_stations):\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            station_type = random.choice(self.station_types)\n",
    "            \n",
    "            stations.append({\n",
    "                \"id\": f\"station_{i}\",\n",
    "                \"x\": x, \"y\": y,\n",
    "                \"type\": station_type,\n",
    "                \"is_transfer\": False\n",
    "            })\n",
    "        return stations\n",
    "    \n",
    "    def _generate_connections(self, stations: List[Dict]) -> List[Dict]:\n",
    "        connections = []\n",
    "        for i, s1 in enumerate(stations):\n",
    "            for j, s2 in enumerate(stations[i+1:], i+1):\n",
    "                dist = abs(s1[\"x\"] - s2[\"x\"]) + abs(s1[\"y\"] - s2[\"y\"])\n",
    "                if dist <= 3:\n",
    "                    connections.append({\n",
    "                        \"from\": s1[\"id\"],\n",
    "                        \"to\": s2[\"id\"],\n",
    "                        \"walk_time\": dist * 60\n",
    "                    })\n",
    "        return connections\n",
    "    \n",
    "    def _generate_routes(self, stations: List[Dict]) -> List[Dict]:\n",
    "        routes = []\n",
    "        station_groups = defaultdict(list)\n",
    "        \n",
    "        for station in stations:\n",
    "            station_groups[station[\"type\"]].append(station)\n",
    "        \n",
    "        route_id = 0\n",
    "        for mode, mode_stations in station_groups.items():\n",
    "            if len(mode_stations) >= 2:\n",
    "                routes.append({\n",
    "                    \"id\": f\"route_{route_id}\",\n",
    "                    \"mode\": mode,\n",
    "                    \"stations\": [s[\"id\"] for s in mode_stations[:4]],\n",
    "                    \"color\": f\"#{''.join([random.choice('0123456789ABCDEF') for _ in range(6)])}\"\n",
    "                })\n",
    "                route_id += 1\n",
    "        \n",
    "        return routes\n",
    "\n",
    "print(\"City Builder ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf38793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transit Environment for RL training\n",
    "class TransitEnv(gym.Env):\n",
    "    def __init__(self, grid_size=8, training_cities=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.city_builder = CityBuilder(grid_size)\n",
    "        self.training_cities = training_cities or []\n",
    "        self.city_index = 0\n",
    "        \n",
    "        # Action space: choose route frequency for each route (0-10)\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=0, high=10, shape=(10,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space: city state\n",
    "        obs_size = grid_size * grid_size * 4  # zones + stations + routes\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1, shape=(obs_size,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Use pre-generated cities if available, otherwise generate new\n",
    "        if self.training_cities:\n",
    "            self.city_data = self.training_cities[self.city_index % len(self.training_cities)]\n",
    "            self.city_index += 1\n",
    "        else:\n",
    "            self.city_data = self.city_builder.generate_random_city()\n",
    "            \n",
    "        self.current_step = 0\n",
    "        self.max_steps = 100\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        return obs, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply actions (route frequencies)\n",
    "        route_frequencies = action\n",
    "        \n",
    "        # Calculate reward based on efficiency\n",
    "        reward = self._calculate_reward(route_frequencies)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        truncated = False\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        return obs, reward, done, truncated, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        # Convert city data to observation vector\n",
    "        obs = np.zeros(self.grid_size * self.grid_size * 4, dtype=np.float32)\n",
    "        \n",
    "        # Encode zones\n",
    "        for zone in self.city_data[\"zones\"]:\n",
    "            idx = zone[\"x\"] * self.grid_size + zone[\"y\"]\n",
    "            if zone[\"type\"] == \"residential\":\n",
    "                obs[idx] = 1.0\n",
    "            elif zone[\"type\"] == \"commercial\":\n",
    "                obs[idx + self.grid_size * self.grid_size] = 1.0\n",
    "            elif zone[\"type\"] == \"industrial\":\n",
    "                obs[idx + 2 * self.grid_size * self.grid_size] = 1.0\n",
    "        \n",
    "        # Encode stations\n",
    "        for station in self.city_data[\"stations\"]:\n",
    "            idx = station[\"x\"] * self.grid_size + station[\"y\"]\n",
    "            obs[idx + 3 * self.grid_size * self.grid_size] = 1.0\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _calculate_reward(self, route_frequencies):\n",
    "        # Simple reward: efficiency vs cost\n",
    "        efficiency = np.sum(route_frequencies) * 0.1\n",
    "        cost = np.sum(route_frequencies ** 2) * 0.05\n",
    "        coverage = len(self.city_data[\"stations\"]) * 0.02\n",
    "        \n",
    "        reward = efficiency + coverage - cost\n",
    "        return float(reward)\n",
    "\n",
    "print(\"Transit Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training cities\n",
    "print(\"Generating training cities...\")\n",
    "builder = CityBuilder(8)\n",
    "training_cities = []\n",
    "\n",
    "for i in range(50):\n",
    "    city = builder.generate_random_city()\n",
    "    training_cities.append(city)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Generated {i+1} cities\")\n",
    "\n",
    "print(f\"Generated {len(training_cities)} training cities\")\n",
    "print(f\"Sample city has {len(training_cities[0]['stations'])} stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d60595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with training cities\n",
    "env = TransitEnv(grid_size=8, training_cities=training_cities)\n",
    "print(f\"Environment created with {len(training_cities)} training cities\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# Test environment\n",
    "obs, info = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Current city has {len(env.city_data['stations'])} stations\")\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(f\"Test step reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745016ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./logs/\"\n",
    ")\n",
    "\n",
    "print(f\"PPO model created on {device}\")\n",
    "print(f\"Policy network: {model.policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff00dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "total_timesteps = 100000\n",
    "save_freq = 10000\n",
    "\n",
    "# Create callback for evaluation\n",
    "eval_env = TransitEnv(grid_size=8)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    best_model_save_path=\"./models/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=eval_callback,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee968c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save(\"transit_ppo_final\")\n",
    "print(\"Model saved as 'transit_ppo_final'\")\n",
    "\n",
    "# Test trained model\n",
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Test episode reward: {total_reward}\")\n",
    "print(f\"Average reward per step: {total_reward / (step + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluate on multiple episodes\n",
    "num_eval_episodes = 20\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_eval_episodes):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(100):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    if episode % 5 == 0:\n",
    "        print(f\"Episode {episode}: {episode_reward:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rewards, bins=10)\n",
    "plt.title(\"Reward Distribution\")\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average reward: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}\")\n",
    "print(f\"Min reward: {np.min(rewards):.2f}\")\n",
    "print(f\"Max reward: {np.max(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a203dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model analysis\n",
    "print(\"Model Analysis:\")\n",
    "print(f\"Policy network: {model.policy}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.policy.parameters())}\")\n",
    "\n",
    "# Sample action analysis\n",
    "obs, info = env.reset()\n",
    "action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "print(f\"\\nSample action (route frequencies): {action}\")\n",
    "print(f\"Action statistics:\")\n",
    "print(f\"  Mean: {np.mean(action):.3f}\")\n",
    "print(f\"  Std: {np.std(action):.3f}\")\n",
    "print(f\"  Min: {np.min(action):.3f}\")\n",
    "print(f\"  Max: {np.max(action):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1667fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model files (for Colab)\n",
    "from google.colab import files\n",
    "\n",
    "try:\n",
    "    # Zip model files\n",
    "    !zip -r trained_models.zip transit_ppo_final.zip models/ logs/\n",
    "    \n",
    "    # Download\n",
    "    files.download('trained_models.zip')\n",
    "    print(\"Model files downloaded!\")\n",
    "except:\n",
    "    print(\"Not running on Colab, skipping download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cc044",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "The RL agent has been trained on synthetic transit data. Key results:\n",
    "\n",
    "- **Environment**: 8x8 grid cities with multiple transit modes\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Training**: 100K timesteps with GPU acceleration\n",
    "- **Objective**: Optimize route frequencies for efficiency\n",
    "\n",
    "The trained model can now make decisions about transit route scheduling to maximize efficiency while minimizing costs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
